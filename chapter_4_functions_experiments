################################################################################
# This script contains the finctions used to generate the most relevant 
# explanations. The script includes additional code to test the average
# neighbourhood size. This script forms the foundation of the functions included
# in the XBN package developed in this thesis. 

################################################################################
# required libraries

require(gRain)
require(plyr)
require(gtools)
require(glassoFast)

################################################################################
odds <- function(p) {
  
  # function to compute the odds
  
  p/(1 - p)
}
################################################################################
split_names <- function(names) {
  
  # function to split names into nodes and states
  
  node_parts <- gsub("__.*$", "", names)
  state_parts <- gsub("^[^_]*__", "", names)
  list(nodes = node_parts, states = state_parts)
}
################################################################################
init_gbf <- function(target_set, evidence_set, evidence_states, bn_grain) {
  
  # function to compute the starting values for each target in the set
  
  # target_set = set of variables we want to explore to explain observed evidence
  # evidence_set = set of observed variables, for example: dyspnoea
  # evidence_states = observed states of evidence variables, for example: true
  # bn_grain = Bayesian network converted into a junction tree representation
  #               class(bn_grain): "cpt_grain" "grain"
  
  # create empty set to save gbf scores for each target
  gbf_results <- c()
  
  # set observed evidence
  e_1 <- setEvidence(bn_grain, nodes = evidence_set, states = evidence_states)
  
  # the `best pivot` rule requires each target variable to be in their most 
  # optimal state. 
  # for each starting solution set t:
  for (t in target_set) {
    
    # numerator: observed hypotheses
    # query, i.e., obtain the conditional distribution
    odds_1 <- odds(querygrain(e_1, nodes = t, type = 'joint'))
    
    # denominator: all alternative hypotheses
    # query, i.e., obtain the conditional distribution
    odds_2 <- odds(querygrain(bn_grain, nodes = t, type = 'joint'))
    
    # generalised bayes factor
    gbf <- odds_1 / odds_2
    gbf_df <- as.data.frame.table(gbf)
    best_gbf <- gbf_df[which.max(gbf_df$Freq),]
    
    gbf_results <- rbind.fill(gbf_results, best_gbf)
    # Reorder the columns, moving the specified column to the last position
    gbf_results <- gbf_results[, c(setdiff(names(gbf_results), 'Freq'), 'Freq')]
    
  }
  
  # rename `Freq` to `GBF`
  names(gbf_results)[names(gbf_results) == 'Freq'] <- 'GBF'
  
  # arrange the starting solutions in descending order
  gbf_results <- gbf_results %>%
    arrange(desc(GBF)) %>%
    
    # return the starting solutions for each target variable in target_set
    return(gbf_results)
  
}
################################################################################

gbf_set = function(sol_set, evidence_set, evidence_states, bn_grain) {
  
  # function to compute the gbf scores for each solution in sol_set
  
  # sol_set = set of variables we want to explore to explain observed evidence
  # evidence_set = set of observed variables, for example: dyspnoea
  # evidence_states = observed states of evidence variables, for example: true
  # bn_grain = Bayesian network converted into a junction tree representation
  #               class(bn_grain): "cpt_grain" "grain"
  
  # create empty set to save generalised bayes factor (gbf) scores for each target
  gbf_results <- c()
  
  # set observed evidence
  e_1 <- setEvidence(bn_grain, nodes = evidence_set, states = evidence_states)
  
  # for row in sol_set:
  for (j in 1:nrow(sol_set)) {
    
    # extract node j from sol_set
    node_input <- as.character(sol_set[j,])
    
    # numerator: observed hypotheses
    # query, i.e., obtain the conditional distribution
    odds_1 <- odds(querygrain(e_1, nodes = node_input, type = "joint"))
    
    # denominator: all alternative hypotheses
    # query, i.e., obtain the conditional distribution
    odds_2 <- odds(querygrain(bn_grain, nodes = node_input, type = "joint"))
    
    # generalised bayes factor
    gbf <- odds_1 / odds_2
    gbf_df <- as.data.frame.table(gbf)
    
    gbf_results <- rbind.fill(gbf_results, gbf_df)
    # Reorder the columns, moving the specified column to the last position
    gbf_results <- gbf_results[, c(setdiff(names(gbf_results), 'Freq'), 'Freq')]
  }
  
  # rename `Freq` to `GBF`
  names(gbf_results)[names(gbf_results) == 'Freq'] <- 'GBF'
  
  # return the gbf scores for all variables in sol_set
  return(gbf_results)
  
}

################################################################################
compare_gbf <- function(y_gbf, neighbour_gbf) {
  
  # function to compare the current best solution with best neighbour
  
  # y_gbf: current best solution
  # neighbour_gbf: top neighbouring solution
  
  gbf_sol <- data.frame()
  
  # if generalised Bayes factor of y_gbf is greater than or equal to
  # generalised Bayes factor of neighbour_gbf
  if (y_gbf$GBF >= neighbour_gbf[1,]$GBF) {
    # keep y_gbf as best solution
    updated_y <- y_gbf
  }
  else {
    # update current best to neighbour_gbf
    updated_y <- neighbour_gbf[1,]
  }
  
  # populate updated best solution
  gbf_sol <- rbind.fill(gbf_sol, updated_y)
  
  # remove duplicates
  gbf_sol <- gbf_sol[!duplicated(gbf_sol),]
  
  return(gbf_sol)
  
}

################################################################################
minimal_exp <- function(best_set) {
  
  # function to return minimal set based on dominance relations
  
  # best_set = set of most relevant explanations
  
  not_minimal_df <- data.frame()
  minimal_df <- data.frame()
  
  minimal_set <- data.frame()
  
  n <- nrow(best_set)
  
  best_set_targets <- best_set
  best_set_targets[, c('GBF')] <- list(NULL)
  
  best_set_cols <- colnames(best_set)
  # best_set_cols[, c('mre_size')] <- list(NULL)
  
  for (i in 1:n) {
    
    # x
    x <- best_set[i, ]
    x_n <- best_set_targets[i, ]
    x_names <- names(which(sapply(x_n, function(x) any(!is.na(x)))))
    x_length <- length(which(sapply(x_n, function(x) any(!is.na(x))))) # non NA length
    
    for (j in 1:n) {
      
      if (i == j) next
      
      # y
      y <- best_set[j,]
      y_n <- best_set_targets[j, ]
      y_names <- names(which(sapply(y_n, function(x) any(!is.na(x)))))
      y_length <- length(which(sapply(y_n, function(x) any(!is.na(x))))) # non NA length
      
      # dom_relation: dominance relation
      # 1: remove solutions that are strongly dominated by another
      # 2: minimal
      # 3: not in above categories
      
      # check if x_names is a subset of y_names
      x_subset_y_names <- all(x_names %in% y_names)
      # check if x contains less variables than y
      x_subset_y_length <- (x_length < y_length)
      # check if x_names is a superset of y_names
      x_superset_y_names <- all(y_names %in% x_names)
      # check if x contains more variables than y
      x_superset_y_length <- (x_length > y_length)
      
      # logical vector showing matching variables in x and y
      x_match_y <- (x_n == y_n)
      # count of all matches (TRUE)
      x_match_y_true <- sum(x_n == y_n, na.rm = TRUE) 
      # count of all NA
      x_match_y_na <- sum(is.na(x_n == y_n)) 
      # count of all non-matches (FALSE)
      # We use this to check if x is a subset/superset of y. 
      # If there are more than 0 FALSE, x not a subset/superset of y.
      x_match_y_false <- (length(x_match_y) - x_match_y_true - x_match_y_na) 
      
      if (x_subset_y_names && x_match_y_false == 0) {
        if (x_subset_y_length) {
          # x subset of y
          if (x$GBF >= y$GBF) {
            # x strongly dominates y. keep x
            dom_relation = '2'
          } else {
            # x does not strongly dominate y. Discard x
            dom_relation = '1'
          }
        } else {
          # x not subset/superset of y
          # x and y consist of the exactly same variables, however, some states differ
          # x not dominated by another explanation. Keep x
          dom_relation <- 2
        }
      } else if (x_superset_y_names && x_match_y_false == 0 && x_superset_y_length) {
        # x superset of y
        if (x$GBF > y$GBF) {
          # x weakly dominates y. Keep x
          dom_relation <- '2'
        } else {
          # x does not weakly dominate y. Discard x
          dom_relation <- '1'
        }
      } else {
        # x not subset/superset of y
        # x and y differ by variables + states.
        # x not dominated by another explanation. Keep x
        dom_relation <- '2'
      }
      
      if (dom_relation == '1') {
        not_minimal_df <- rbind.fill(not_minimal_df, x)
      } else if (dom_relation == '2') {
        minimal_df <- rbind.fill(minimal_df, x)
      }
      
      # remove duplicated
      not_minimal_df <- unique(not_minimal_df)
      minimal_df <- unique(minimal_df)
      
      if (nrow(not_minimal_df) > 0 && nrow(minimal_df) > 0) {
        minimal_set <- anti_join(minimal_df, not_minimal_df, by = best_set_cols)
      } else {
        minimal_set <- minimal_df
      }
      
    }
  }
  
  return(minimal_set)
  
}
################################################################################
score_matrix_fun <- function(df_target_neighbours, my_gbfs) {
  
  # create score_matrix -- this is used to compute sigma for glasso
  # elements in df_target_neighbours$neighbours == row and column names of score_matrix
  matrix_cols <- df_target_neighbours$neighbours
  # the number of rows in df_target_neighbours will determine the dimension of score_matrix
  matrix_dim <- nrow(df_target_neighbours)
  # create score_matrix filled with zero's
  score_matrix <- matrix(0, nrow = matrix_dim,
                         ncol = matrix_dim,
                         dimnames = list(matrix_cols, matrix_cols))
  
  # Preprocess row and column names
  row_names <- strsplit(rownames(score_matrix), " ")
  # col_names <- row_names
  
  # Extract node and state information from names
  row_info <- lapply(row_names, split_names)
  
  # Calculate row sums outside the loop
  row_sums <- rowSums(!is.na(my_gbfs))
  
  # Iterate over rows and columns
  for (r in seq_len(matrix_dim)) {
    
    node_r <- row_info[[r]]$nodes
    state_r <- row_info[[r]]$states
    row_names_r <- row_names[[r]]
    
    for (c in r:matrix_dim) {
      
      node_c <- row_info[[c]]$nodes
      state_c <- row_info[[c]]$states
      col_names_c <- row_names[[c]]
      
      common_rows <- !is.na(match(row_names_r, col_names_c))
      common_cols <- !is.na(match(col_names_c, row_names_r))
      
      state_set <- c(state_r, state_c)
      node_set <- c(node_r, node_c)
      
      if (any(common_rows) || any(common_cols)) {
        
        length_diff_cr <- length(setdiff(col_names_c, row_names_r))
        
        if (length_diff_cr == 0) {
          state_set <- state_r
          node_set <- node_r
        } else if (length_diff_cr == 1) {
          rc <- unique(c(row_names_r, col_names_c))
          node_set <- substr(rc, 1, regexpr("__", rc) - 1)
          state_set <- substr(rc, regexpr("__", rc) + 1, nchar(rc))
        }
      } else if (any(node_c %in% node_r)) {
        state_set <- NULL
        node_set <- NULL
      }
      
      length_node_set <- length(node_set)
      c_match <- colnames(my_gbfs) %in% node_set
      
      row_match2 <- FALSE
      
      if (any(c_match)) {
        subset_all_combos_gbf <- my_gbfs[row_sums - 1 == length_node_set, c("GBF", colnames(my_gbfs)[c_match])]
        subset_all_combos_gbf <- subset_all_combos_gbf[complete.cases(subset_all_combos_gbf), ]
        
        if (length_node_set != 1 || !any(node_r == node_c) || !any(state_r == state_c)) {
          row_match <- t(apply(subset_all_combos_gbf[, node_set], 1,
                               function(x) x == state_set))
          row_match2 <- rowSums(row_match) == length_node_set
        } else {
          row_match <- subset_all_combos_gbf[, node_set] == state_set
          row_match2 <- sapply(row_match, function(x) all(x))
        }
        
        if (any(row_match2)) {
          score_matrix[r, c] <- subset_all_combos_gbf[row_match2, "GBF"][1]
          
        }
      }
      
    }
    
  }
  
  # Assign values symmetrically
  score_matrix[lower.tri(score_matrix)] <- t(score_matrix)[lower.tri(score_matrix)]
  
  return(score_matrix)
  
}

################################################################################
neighbours <- function(a1_neighbour_preglasso, c1_neighbour_preglasso, target_set_length, glasso_result_inv) {
  
  preglasso_neighbours <- data.frame(neighbours = c(a1_neighbour_preglasso, c1_neighbour_preglasso))
  
  # preglasso_neighbours <- data.frame(neighbours = c(a1_neighbour_preglasso, c1_neighbour_preglasso), stringsAsFactors = FALSE)
  colnames(preglasso_neighbours) <- c('neighbours')
  preglasso_neighbours_rows <- nrow(preglasso_neighbours)
  
  all_preglasso_neighbours <- as.data.frame(matrix(NA_character_, nrow = preglasso_neighbours_rows, ncol = target_set_length))
  colnames(all_preglasso_neighbours) <- target_set
  
  glasso_subset <- cbind(all_preglasso_neighbours, glasso_inv = character(preglasso_neighbours_rows))
  
  neighbour_combos_split <- strsplit(preglasso_neighbours$neighbours, " ")
  
  for (i in seq_len(preglasso_neighbours_rows)) {
    combos_len <- length(neighbour_combos_split[[i]])
    combos_len_u <- length(unique(neighbour_combos_split[[i]]))
    
    rows <- neighbour_combos_split[[i]][seq_len(combos_len - 1)]
    cols <- neighbour_combos_split[[i]][combos_len]
    
    r_node <- stri_replace_last_regex(rows, "__.*$", "")
    r_state <- stri_replace_first_regex(rows, ".*__", "")
    
    c_node <- if (combos_len_u > 1) stri_replace_last_regex(cols, "__.*$", "") else NULL
    c_state <- if (combos_len_u > 1) stri_replace_first_regex(cols, ".*__", "") else NULL
    
    rc_node <- c(r_node, c_node)
    rc_state <- c(r_state, c_state)
    
    all_preglasso_neighbours[i, match(rc_node, colnames(all_preglasso_neighbours))] <- rc_state
    
    glasso_subset[i, ] <- all_preglasso_neighbours[i, ]
    glasso_subset[i, 'glasso_inv'] <- glasso_result_inv[paste(rows, collapse = " "), cols]
  }
  
  glasso_subset <- glasso_subset[glasso_subset[, "glasso_inv"] != 0, ]
  
  return(list(all_preglasso_neighbours = all_preglasso_neighbours,
              glasso_subset = glasso_subset))
  
}

################################################################################
mre_brute <- function(target_set, evidence_set, evidence_states, bn_grain) {
  
  # brute force function to obtain all explanations
  
  # target_set = set of variables we want to explore to explain observed evidence
  # evidence_set = set of observed variables, for example: dyspnoea
  # evidence_states = observed states of evidence variables, for example: true
  # bn_grain = Bayesian network converted into a junction tree representation
  
  gbf_table <- c()
  
  # set observed evidence
  e_1 <- setEvidence(bn_grain, nodes = evidence_set, states = evidence_states)
  
  for (i in 1:length(target_set)) {
    X1 <- combinations(n = length(target_set), r = i, v = target_set, set = T, 
                       repeats.allowed = F)
    node_record <- data.frame(X1)
    
    for (j in 1:nrow(node_record)) {
      
      node_input <- as.character(node_record[j,])
      
      # numerator: observed hypotheses
      # query, i.e., obtain the conditional distribution
      odds_1 <- odds(querygrain(e_1, nodes = node_input, type = "joint"))
      
      # denominator: all alternative hypotheses
      # query, i.e., obtain the conditional distribution
      odds_2 <- odds(querygrain(bn_grain, nodes = node_input, type = "joint"))
      
      # generalised bayes factor
      gbf <- (odds_1 / odds_2)
      
      gbf_df <- as.data.frame.table(gbf)
      gbf_table <- rbind.fill(gbf_table, gbf_df)
      
      # Reorder the columns, moving the specified column to the last position
      gbf_table <- gbf_table[, c(setdiff(names(gbf_table), 'Freq'), 'Freq')]
      
    }
  }
  
  # rename `Freq` to `GBF`
  names(gbf_table)[names(gbf_table) == 'Freq'] <- 'GBF'
  
  # arrange the starting solutions in descending order
  gbf_table <- gbf_table %>%
    distinct() %>%
    drop_na(GBF) %>%
    arrange(desc(GBF)) %>%
    select(-GBF,everything()) %>%
    mutate(mre_size = rowSums(!is.na(.)) - 1)
  
  gbf_table <- gbf_table %>%
    mutate_at(vars(all_of(target_set)), as.character)
  
  return(gbf_table)
  
}

################################################################################

mre_fwd <- function(target_set, evidence_set, evidence_states, bn_grain) {
  
  # Function to solve the most relevant explanation in Bayesian networks using
  # a Forward search algorithm. 
  
  # Input:
  #  target_set = set of variables we want to explore to explain observed evidence
  #  evidence_set = set of observed variables. 
  #  evidence_states = observed states of evidence variables
  #  bn_grain = Bayesian network converted into a junction tree representation
  # Output:
  #  mre_set = set of most relevant explanations
  
  # initialise the starting solution set, I
  # init.GBF based on best pivot rule
  i_set <- init_gbf(target_set = target_set,
                    evidence_set = evidence_set,
                    evidence_states = evidence_states,
                    bn_grain = bn_grain)
  
  # initialise the current best solution
  y_best <- data.frame()
  
  # create empty set to save all candidate solutions
  candidate_solutions <- data.frame()
  
  # obtain length of target set
  target_set_length <- length(target_set)
  
  # create empty set to save all combinations visited
  # certain combinations may be visited more than once
  all_combos_visit <- data.frame(matrix(NA, ncol = target_set_length))
  all_combos_visit_cnames <- colnames(all_combos_visit)
  
  all_neighbours_calc_gbf <- c()
  
  # empty set for skipping
  empty_gbf <- data.frame(matrix(NA, ncol = ncol(i_set)))
  colnames(empty_gbf) <- colnames(i_set)
  
  # count neighbour size
  num_neighbours <- 0
  
  # for each starting solution do:
  for (s_id in 1:nrow(i_set)) {
    
    # s: single starting solution in I
    s <- i_set[s_id,]
    # set y = s
    y <- s
    
    # the process will repeat until y no longer updates
    while (!is.infinite(y$GBF)) {
      
      #######################
      # check this
      # if (is.infinite(y$GBF)) next
      
      #######################
      
      # get variable names present in y
      # remove `GBF` column from y since we only want the names of the variables
      y_target_nogbf <- y[-length(y)]
      # obtain names of variables present in y. This will exclude those variable 
      # names that are NA-cells
      y_target <- names(which(sapply(y_target_nogbf, function(x) any(!is.na(x)))))
      y_target <- as.data.frame(y_target)
      
      # get name + "_" + state of target node(s). This is used to set up score_matrix
      # create empty set to save all name + "_" + state combinations
      targets_name_state <- c()
      # convert y_target_nogbf list to vector
      y_target_vec <- y_target_nogbf[unlist(y_target)]
      for (col_name in colnames(y_target_vec)) {
        cell_value <- unlist(y_target_vec[col_name])
        # get name + "_" + state of target node
        target_name_state <- paste(names(cell_value), cell_value[1], sep = "__")
        # save all name + "_" + state of target nodes
        targets_name_state <- c(targets_name_state, target_name_state)
      }
      
      # concatenate elements in targets_name_state into single string
      target_name_state_s <- paste(targets_name_state, collapse = " ")
      
      # the (starting) solution is improved by either adding an additional feature
      # or by changing the state of an existing feature in the solution
      
      # `add one` node as neighbour:
      y_target_unlist <- unlist(y_target)
      # select nodes from target_set not present in y_target (y_best)
      a1_nodes <- setdiff(target_set, y_target_unlist)
      
      # save target + `add one`
      a1_neighb <- paste(paste(names(y_target_vec), collapse = " "), a1_nodes, sep = " ")
      
      # create empty set to save all `add one` neighbours
      a1_neighbours <- c()
      
      # add states for each node in a1_nodes
      for (i in a1_nodes) {
        # extract states from bn_grain for node i
        a1_state <- bn_grain$universe$levels[[i]]
        # concatenate node i + "_" + a1_state
        a1_node_state <- paste(i, a1_state, sep = "__")
        # concatenate target + a1_node_state
        a1_neighbour <- paste(target_name_state_s, a1_node_state, sep = " ")
        # save `add one` neighbours
        a1_neighbours <- c(a1_neighbours, a1_neighbour)
      }
      
      # `change one` node as a neighbour
      # create empty set to save all `change one` neighbours
      c1_neighbours <- c()
      
      # for each node in the target set
      for (i in 1:ncol(y_target_vec)) {
        # extract states from bn_grain for node i
        c1_state <- bn_grain$universe$levels[[names(y_target_vec[i])]]
        # make copy of y_target_vec
        c1_state_change <- y_target_vec
        # extract states not included in y_target_vec
        c1_state_diff <- setdiff(c1_state, y_target_vec[,i])
        
        # iterate through all states in c1_state_diff
        for (j in 1:length(c1_state_diff)) {
          
          c1_state_change[,i] <- c1_state_diff[j]
          # extract node name
          c1_nodes <- names(c1_state_change)
          # concatenate nodes in c1_nodes -- this is used to get gbf of neighbours
          c1_nodes_concat <- paste(c1_nodes, collapse = " ")
          # concatenate node + "_" + state
          c1_neighbour1 <- paste(c1_nodes, format(c1_state_change), sep = "__")
          # concatenate elements in c1_neighbour
          c1_neighbour <- paste(c1_neighbour1, collapse = " ")
          # save `change one` neighbours
          c1_neighbours <- rbind(c1_neighbours, c1_neighbour)
        }
      }
      
      # save `add one` and `change one` neighbours into a data.frame
      df_neighbours <- data.frame(c(t(a1_neighbours),
                                    t(c1_neighbours)))
      colnames(df_neighbours) <- c('neighbours')
      
      # keep only unique rows in df_neighbours
      df_neighbours <- unique(df_neighbours)
      
      # get nodes for generalised bayes factor calculations
      # create data.frame from empty matrix with,
      # number of columns  = number of target variables
      # number of rows = number of neighbours
      all_neighbours_calc <- data.frame(matrix(NA,
                                               ncol = target_set_length,
                                               nrow = (length(a1_nodes)) + length(c1_neighbour)))
      
      df_neighbours_calc <- data.frame(c(t(a1_neighb), c1_nodes_concat))
      colnames(df_neighbours_calc) <- c("neighbours")
      
      for (i in 1:nrow(all_neighbours_calc)) {
        n <- length(strsplit(df_neighbours_calc[i,], " ")[[1]])
        for (j in 1:n) {
          all_neighbours_calc[i,j] <- strsplit(df_neighbours_calc[i,], " ")[[1]][j]
        }
      }
      
      # compute gbf score for all neighbours
      all_neighbours_calc <- setdiff(all_neighbours_calc, all_combos_visit)
      
      # update combinations already visited
      all_combos_visit <- unique(rbind(all_combos_visit, all_neighbours_calc))
      
      # compute gbf score for combinations not yet visited 
      filtered_combos_gbf <- empty_gbf
      if (nrow(all_neighbours_calc) > 0) {
        filtered_combos_gbf <- gbf_set(sol_set = all_neighbours_calc,
                                       evidence_set = evidence_set, 
                                       evidence_states = evidence_states,
                                       bn_grain = bn_grain)
      }
      
      filtered_combos_gbf <- unique(filtered_combos_gbf)
      all_neighbours_calc_gbf <- unique(rbind.fill(all_neighbours_calc_gbf, filtered_combos_gbf))
      all_neighbours_calc_gbf <- all_neighbours_calc_gbf[!apply(is.na(all_neighbours_calc_gbf), 1, all), ]
      
      
      neighbour_subset <- data.frame(matrix(NA,
                                            ncol = target_set_length,
                                            nrow = (length(df_neighbours))))
      colnames(neighbour_subset) <- target_set
      
      for (i in 1:nrow(df_neighbours)) {
        
        # extract row
        neighbour_i <- strsplit(df_neighbours[i,], " ")[[1]]
        
        # extract nodes
        neighbour_i_n <- stri_replace_last_regex(neighbour_i, "__.*$", "")
        
        # extract states
        neighbour_i_s <- stri_replace_first_regex(neighbour_i, ".*__", "")
        
        neighbour_subset[i, neighbour_i_n] <- neighbour_i_s
      }
      
      
      # select only those values from all_neighbours_calc_gbf that occur in df_neighbours
      neighbour_subset_gbf <- semi_join(all_neighbours_calc_gbf, neighbour_subset, by = target_set)
      
      neighbour_subset_gbf <- neighbour_subset_gbf[!is.na(neighbour_subset_gbf$GBF), ]
      
      all_neighbours_gbf <- bind_rows(filter(neighbour_subset_gbf, !is.nan(GBF)), candidate_solutions)
      all_neighbours_gbf <- minimal_exp(all_neighbours_gbf)
      
      # check if neighbouring set is in candidate solutions. If so, remove set from
      # neighbouring set
      if (nrow(candidate_solutions) >= 1) {
        all_neighbours_gbf <- anti_join(all_neighbours_gbf, candidate_solutions, by = target_set)
      } 
      
      # arrange neighbours according to generalised bayes factor
      all_neighbours_gbf <- all_neighbours_gbf[order(-all_neighbours_gbf$GBF), ]
      
      num_neighbours <- num_neighbours + nrow(all_neighbours_gbf)
      
      # compare y_best with neighbours
      if (nrow(all_neighbours_gbf) >= 1) {
        y_update <- compare_gbf(y, all_neighbours_gbf[1,])
      } else {
        y_update = y
      }
      
      # save previous y for stop criteria
      y_old <- y
      # update y
      y <- y_update
      
      # save all candidate solutions visited
      candidate_solutions <- unique(rbind.fill(candidate_solutions, y))
      
      # end repeat or while loop
      if (setequal(y, y_old) || sum(is.na(y)) == 0) {
        break
      }
    }
    
    # update y_best
    y_best <- unique(rbind.fill(y_best, y))
    
  }
  
  # arrange solution set according to generalised bayes factor score
  mre_set <- y_best %>%
    distinct(GBF, .keep_all = TRUE) %>%
    arrange(desc(GBF)) %>%
    select(-GBF, everything()) %>%
    mutate(mre_size = rowSums(!is.na(.)) - 1)
  
  mre_set <- mre_set %>%
    mutate_at(vars(all_of(target_set)), as.character)
  
  return(list(mre_set, num_neighbours))
  
}

################################################################################

mre_fwd_glasso <- function(target_set, evidence_set, evidence_states, bn_grain,
                           bn_rho, score_scale = TRUE) {
  
  # Function to solve the most relevant explanation in Bayesian networks using
  # a Forward-gLasso search algorithm. 
  
  # Input:
  #  target_set = set of variables we want to explore to explain observed evidence
  #  evidence_set = set of observed variables. 
  #  evidence_states = observed states of evidence variables
  #  bn_grain = Bayesian network converted into a junction tree representation
  #  bn_rho = The regularization parameter for lasso.
  # Output:
  #  mre_set = set of most relevant explanations
  
  # initialise the starting solution set, I
  # init.GBF based on best pivot rule
  
  i_set <- init_gbf(target_set = target_set,
                    evidence_set = evidence_set,
                    evidence_states = evidence_states,
                    bn_grain = bn_grain)
  
  # initialise the current best solution
  y_best <- data.frame()
  
  # create empty set to save all candidate solutions
  candidate_solutions <- data.frame()
  
  # obtain length of target set
  target_set_length <- length(target_set)
  
  # create empty set to save all combinations visited
  # certain combinations may be visited more than once
  all_combos_visit <- data.frame(matrix(NA, ncol = target_set_length))
  all_combos_visit_cnames <- colnames(all_combos_visit)
  
  # empty set for skipping
  empty_gbf <- data.frame(matrix(NA, ncol = ncol(i_set)))
  colnames(empty_gbf) <- colnames(i_set)
  
  # empty set for saving combination solutions
  my_gbfs <- c()
  
  # count neighbour size
  num_neighbours <- 0
  
  # for each starting solution do:
  for (s.id in seq_len(nrow(i_set))) {
    
    # s: single starting solution in I
    s <- i_set[s.id,]
    # set y = s
    y <- s
    
    # the process will repeat until y no longer updates
    # repeat {}
    while (!is.infinite(y$GBF)) {
      
      #######################
      # check this
      # if (is.infinite(y[['GBF']])) next
      
      #######################
      
      # get variable names present in y
      # remove `GBF` column from y since we only want the names of the variables
      y_target_nogbf <- y[-length(y)]
      # obtain names of variables present in y. This will exclude those variable 
      # names that are NA-cells
      y_target <- names(which(sapply(y_target_nogbf, function(x) any(!is.na(x)))))
      y_target <- as.data.frame(y_target)
      
      # get name + "_" + state of target node(s). This is used to set up score_matrix
      # create empty set to save all name + "_" + state combinations
      targets_name_state <- c()
      # convert y_target_nogbf list to vector
      y_target_vec <- y_target_nogbf[unlist(y_target)]
      for (col_name in colnames(y_target_vec)) {
        cell_value <- unlist(y_target_vec[col_name])
        # get name + "_" + state of target node
        target_name_state <- paste(names(cell_value), cell_value[1], sep = "__")
        # save all name + "_" + state of target nodes
        targets_name_state <- c(targets_name_state, target_name_state)
      }
      
      # concatenate elements in targets_name_state into single string
      target_name_state_s <- paste(targets_name_state, collapse = " ")
      
      # the (starting) solution is improved by either adding an additional feature
      # or by changing the state of an existing feature in the solution
      
      # `add one` node as neighbour:
      y_target_unlist <- unlist(y_target)
      # select nodes from target_set not present in y_target (y_best)
      a1_nodes <- setdiff(target_set, y_target_unlist)
      
      # create empty vector to save all `add one` neighbours
      a1_neighbours <- character()
      
      # add states for each node in a1_nodes
      for (i in a1_nodes) {
        # extract states from bn_grain for node i
        a1_state <- bn_grain$universe$levels[[i]]
        # concatenate node i + "_" + a1_state
        a1_node_state <- paste(i, a1_state, sep = "__")
        # append to a1_neighbours vector
        a1_neighbours <- c(a1_neighbours, a1_node_state)
      }
      
      # will be used to set up preglasso neighbours
      a1_neighbour_preglasso <- paste(target_name_state_s, a1_neighbours, sep = " ")
      
      # save target node(s) and `add one` neighbours into a data.frame
      df_target_neighbours <- data.frame(c(target_name_state_s, t(a1_neighbours)))
      colnames(df_target_neighbours) <- c('neighbours')
      
      # `change one` node as a neighbour
      # extract the states from bn_grain for all nodes
      c1_states <- lapply(names(y_target_vec), function(node) bn_grain$universe$levels[[node]])
      
      # create empty sets for `change one`.diff and .remain
      # we are `splitting` the `change one` neighbours into two separate elements
      # such that we can obtain the partial correlation coefficients in glasso
      c1_neighbours_diff <- c()
      c1_neighbours_remain <- c()
      
      # for each node in the target set
      for (i in seq_len(ncol(y_target_vec))) {
        
        # make a copy of y_target_vec
        c1_state_change <- y_target_vec
        # extract states not included in c1_state_change
        c1_state_diff <- setdiff(c1_states[[i]], c1_state_change[, i])
        
        # iterate through all states in c1_state_diff
        for (j in seq_along(c1_state_diff)) {
          c1_state_change[, i] <- c1_state_diff[j]
          # concatenate node + "_" + state
          c1_neighbour <- sprintf("%s__%s", names(c1_state_change), format(c1_state_change))
          
          # extract `change one`.diff
          c1_neighbours_split_diff = setdiff(c1_neighbour, targets_name_state)
          # extract `change one`.remain
          c1_neighbours_split_remain <- c1_neighbours_split_diff
          if (length(c1_neighbour) > 1) {
            c1_neighbours_split_remain <- intersect(c1_neighbour, targets_name_state)
          }
          
          # concatenate elements in c1_neighbours_split_diff into single string
          c1_neighbour_name_state_diff <- paste(c1_neighbours_split_diff, collapse = " ")
          # concatenate elements in c1_neighbours_split_remain into single string
          c1_neighbour_name_state_remain <- paste(c1_neighbours_split_remain, collapse = " ")
          
          # save `change one`.diff in a data.frame of its own
          c1_neighbours_diff <- c(c1_neighbours_diff, c1_neighbour_name_state_diff)
          # save `change one`remain in a data.frame of its own
          c1_neighbours_remain <- c(c1_neighbours_remain, c1_neighbour_name_state_remain)
          
          # save updated `change one` neighbours in a data.frame
          c1_neighbour_df <- data.frame(c(c1_neighbour_name_state_diff, c1_neighbour_name_state_remain))
          colnames(c1_neighbour_df) <- c('neighbours')
          # save `change one` neighbours into the same data.frame as target and `add one` neighbours
          df_target_neighbours <- rbind(df_target_neighbours, c1_neighbour_df)
          
        }
        
      }
      
      df_target_neighbours <- unique(df_target_neighbours)
      
      c1_neighbour_preglasso <- paste(c1_neighbours_remain, c1_neighbours_diff, sep = " ")
      
      # split neighbours and extract variable names
      neighbour_names <- unique(sapply(strsplit(df_target_neighbours$neighbours, " "), 
                                       function(x) paste0(sub("__.*$", "", x), collapse = " ")))
      
      # generate data.frame with all combinations
      neighbour_names_df <- unique(data.frame(t(combn(neighbour_names, 2))))
      
      # concatenate the elements of neighbour_names_df
      neighbour_names_all <- paste(neighbour_names_df[,1], neighbour_names_df[,2], sep = " ")
      
      neighbour_names_all <- c(neighbour_names_all, y_target_unlist, a1_nodes)
      names(neighbour_names_all) <- NULL
      
      # Split and obtain unique sorted neighbors
      neighbour_names_all <- str_split(neighbour_names_all, " ")
      neighbour_names_unique <- lapply(neighbour_names_all, function(vec) unique(str_sort(vec)))
      neighbour_names_unique <- unique(neighbour_names_unique)
      
      all_combos <- data.frame(matrix(NA, ncol = target_set_length, nrow = length(neighbour_names_unique)))
      
      # Calculate the number of unique neighbor combinations
      num_combos <- length(neighbour_names_unique)
      
      for (i in seq_len(num_combos)) {
        n <- length(neighbour_names_unique[[i]])
        all_combos[i, 1:n] <- neighbour_names_unique[[i]]
      }
      
      all_combos <- setdiff(all_combos, all_combos_visit)
      
      # update combinations already visited
      all_combos_visit <- unique(rbind(all_combos_visit, all_combos))
      
      # compute gbf score for combinations not yet visited 
      filtered_combos_gbf <- empty_gbf
      if (nrow(all_combos) > 0) {
        filtered_combos_gbf <- gbf_set(sol_set = all_combos,
                                       evidence_set = evidence_set, 
                                       evidence_states = evidence_states,
                                       bn_grain = bn_grain)
      }
      
      filtered_combos_gbf <- unique(filtered_combos_gbf)
      my_gbfs <- unique(rbind.fill(my_gbfs, filtered_combos_gbf))
      my_gbfs <- my_gbfs[!apply(is.na(my_gbfs), 1, all), ]
      
      # generalised bayes factor matrix
      score_matrix <- score_matrix_fun(df_target_neighbours, my_gbfs)
      
      ####################
      # Replace Inf values with the chosen Bayes factor value
      score_matrix[is.infinite(score_matrix)] <- max(my_gbfs$GBF[!is.infinite(my_gbfs$GBF)]) + 1
      
      # Replace NaN values with zero
      score_matrix[is.na(score_matrix)] <- rnorm(1, mean = 0, sd = 0.00001)
      # score_matrix[is.na(score_matrix)] <- 0
      ###################
      
      if (score_scale == TRUE) {
        # add very small random noise to zero diagonal values
        diag_indices <- which(diag(score_matrix) == 0)
        noisy_values <- rnorm(length(diag_indices), mean = 0, sd = 0.00001)
        
        score_matrix[diag_indices, diag_indices] <- noisy_values
        
        score_matrix <- scale(score_matrix)
      }
      
      # obtain covariance matrix, sigma
      sigma <- cov(score_matrix)
      
      # apply glasso
      glasso_result <- glassoFast(sigma, rho = bn_rho)
      glasso_result_inv <- glasso_result$wi
      
      # rename rows and columns
      rownames(glasso_result_inv) <- rownames(score_matrix)
      colnames(glasso_result_inv) <- colnames(score_matrix)
      
      neighbour_result <- neighbours(a1_neighbour_preglasso, c1_neighbour_preglasso, target_set_length, glasso_result_inv)
      glasso_subset <- neighbour_result$glasso_subset
      all_preglasso_neighbours <- neighbour_result$all_preglasso_neighbours 
      
      df_neighbours_preglasso <- left_join(all_preglasso_neighbours, my_gbfs, by = target_set)
      # shrink neighbours using results from glasso_subset
      df_neighbours_update <- semi_join(df_neighbours_preglasso, glasso_subset, by = target_set)
      
      df_neighbours_update <- df_neighbours_update[!is.na(df_neighbours_update$GBF), ]
      
      # check minimal explanations
      df_neighbours_update <- bind_rows(filter(df_neighbours_update, !is.nan(GBF)), candidate_solutions)
      df_neighbours_update <- minimal_exp(df_neighbours_update)
      
      # check if neighbouring set is in candidate solutions. If so, remove set from
      # neighbouring set
      if (nrow(candidate_solutions) >= 1) {
        
        df_neighbours_update <- anti_join(df_neighbours_update, candidate_solutions, by = target_set)
      } 
      
      df_neighbours_update <- df_neighbours_update[order(-df_neighbours_update$GBF), ]
      
      num_neighbours <- num_neighbours + nrow(df_neighbours_update)      
      
      # compare y_best with neighbours
      if (nrow(df_neighbours_update) >= 1) {
        y_update <- compare_gbf(y, df_neighbours_update[1,])
      } else {
        y_update = y
      }
      
      # save previous y for stop criteria
      y_old <- y
      # update y
      y <- y_update
      
      # save all candidate solutions visited
      candidate_solutions <- unique(rbind.fill(candidate_solutions, y))
      
      # end repeat or while loop
      # Check if stop rule is satisfied or there are no missing values
      if (setequal(y, y_old) || sum(is.na(y)) == 0) {
        break
      }
      
    }
    
    # update y_best
    y_best <- unique(rbind.fill(y_best, y))
    
  }
  
  # arrange solution set according to generalised bayes factor score
  mre_set <- y_best %>%
    distinct(GBF, .keep_all = TRUE) %>%
    arrange(desc(GBF)) %>%
    select(-GBF, everything()) %>%
    mutate(mre_size = rowSums(!is.na(.)) - 1)
  
  mre_set <- mre_set %>%
    mutate_at(vars(all_of(target_set)), as.character)
  
  return(list(mre_set, num_neighbours))
  
}
